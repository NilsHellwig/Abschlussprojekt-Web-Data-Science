---
title: "01 Crawl Flight History"
author: "Nils Hellwig"
date: "5/16/2023"
output: html_document
---

## Load Packages

```{r}
library(httr)
library(rvest)
```

## Define Settings

```{r}
# We first define the URLs that serve as the basis for each request.
base_url_depatures <- "https://www.flightera.net/en/airport/Munich/EDDM/departure/"
base_url_arrivals <- "https://www.flightera.net/en/airport/Munich/EDDM/arrival/"

# These are the days that are available on Flightera.net for both departures and arrivals
days_for_crawling <- as.character(seq(as.Date("2019-12-12"), as.Date("2023-04-30"), by = "day"))

# Settings to avoid getting a "To Many Requests" error 
requests_per_pause <- 99
pause_duration <- 2800
```

## Download dataset with depatures as (as .html files)

```{r}
# Loop through all days and scrape the webpage for each day
for (i in 1:length(days_for_crawling)) {
  day <- days_for_crawling[i]
  
  # Select a random hour between 0 and 23 and the url for the request
  hour <- sprintf("%02d", sample(0:23, 1))
  url <- paste0(base_url_depatures, day, "%20", hour, "_00?")
  
  # Send an HTTP GET request to the url and read html
  response <- GET(url)
  content <- content(response, as = "text")
  parsed_html <- read_html(content)
  
  # Write the parsed HTML content to a file
  filename <- paste0("datasets/raw_depatures_dataset/request_", day, "_", hour, "_00.html")
  writeLines(as.character(parsed_html), filename)
  
  # Pause for a random amount of time between 7 and 10 seconds
  Sys.sleep(runif(1, 7, 10))
  
  # Check if the number of requests is a multiple of requests_per_pause. 
  # We need to wait 45 Minutes until we can download the next 99 pages
  if (i %% requests_per_pause == 0) {
    Sys.sleep(pause_duration)
  }
}
```

## Download dataset with arrivals as (as .html files)

```{r}
# Loop through all days and scrape the webpage for each day
for (i in 1:length(days_for_crawling)) {
  day <- days_for_crawling[i]
  
  # Select a random hour between 0 and 23 and the url for the request
  hour <- sprintf("%02d", sample(0:23, 1))
  url <- paste0(base_url_arrivals, day, "%20", hour, "_00?")
  
  # Send an HTTP GET request to the url and read html
  response <- GET(url)
  content <- content(response, as = "text")
  parsed_html <- read_html(content)
  
  # Write the parsed HTML content to a file
  filename <- paste0("datasets/raw_arrivals_dataset/request_", day, "_", hour, "_00.html")
  writeLines(as.character(parsed_html), filename)
  
  # Pause for a random amount of time between 7 and 10 seconds
  Sys.sleep(runif(1, 7, 10))
  
  # Check if the number of requests is a multiple of requests_per_pause. 
  # We need to wait for 45 minutes until we can download the next 99 pages.
  if (i %% requests_per_pause == 0) {
    Sys.sleep(pause_duration)
  }
}
```